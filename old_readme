# HAIDI-Graphs

Tool to help with Training Data Attribution. It can generate sentences on LLMs, extract low-perplexity regions in its completions and count the number of utterances in their training data using [Infinigram](https://infini-gram.io/). It can also generates visualization to see the perplexities of each token and where the low-perplexity regions are. 

The paper is available at https://arxiv.org/abs/2507.01844

This project will be presented at ACL 2025, in Vienna. 

![Image](images/sample_viz.png)

## 📁 Project Structure

```
project_root/
├── experiments/                          # Experiment storage
│   ├── experiment_name/
│   │   ├── inference_data/
│   │   │   ├── generations/               # Model outputs with per-token perplexities
│   │   │   │   ├── prompt_001.json
│   │   │   │   ├── prompt_002.json
│   │   │   │   └── ...
│   │   │   └── metadata.json             # Experiment configuration (WIP)
│   │   ├── perplexity_analysis/
│   │   │   ├── low_perp_regions/         # Extracted low-perplexity regions
│   │   │   │   ├── prompt_001_regions.json
│   │   │   │   ├── prompt_002_regions.json
│   │   │   │   └── ...
│   │   │   ├── analysis_config.json      # Analysis parameters (WIP)
│   │   │   └── summary_stats.json        # Aggregate statistics (WIP)
│   │   └── experiment_log.json           # Experiment timeline and metadata
│   ├── exp_2 /
│   └── ...
├── src/
│   ├── infer.py           # Main inference script
│   ├── infinigram.py          # Infinigram API integration
│   ├── plot.py        # HTML visualization toolkit
├── configs/                             # Configuration templates
└── README.md
```

## 🚀 Quick Start

### 1. Installation

```bash
# Install required dependencies
pip install torch transformers requests tqdm matplotlib numpy

# Clone or download the toolkit
git clone https://github.com/Reliable-Information-Lab-HEVS/HAIDI-Graphs
cd HAIDI-Graphs
```

### 2. Basic Perplexity Analysis

This script does inference on the model chosen for the prompts given, stores it in the experiment folder referred and extract the low-perplexity regions. It **does not** count the utterances in the training data. 

```bash
# Single prompt
python src/infer.py \
    --model_name "EleutherAI/pythia-70m" \
    --prompt "The quick brown fox jumps over the lazy dog" \
    --experiment-name "test_experiment" \
    --n_gen 3 \
    --verbose

# Analyze multiple prompts from a JSON file
python src/infer.py \
    --model_name "EleutherAI/pythia-160m" \
    --prompt "prompts.json" \
    --experiment-name "batch_experiment" \
    --n_gen 5 \
    --window-size 8 \
    --perplexity_threshold 1.5
```

### 3. Add Infinigram Corpus Frequencies

This script counts the number of utterances of each low-perp window in the experiment folder. This must be run after `infer`.


```bash
# Process entire experiment directory
python src/infinigram.py experiments/test_experiment/ --confirm --verbose

# Process single file
python src/infinigram.py experiments/test_experiment/perplexity_analysis/low_perp_regions/prompt_001_regions.json
```

### 4. Generate Visualizations

This script generates a HTML file for visualizations. It can be open in a regular browser. 

```bash
# Create HTML visualization for entire experiment
python src/plot.py experiments/test_experiment/ --output results.html

# Visualize specific prompt
python src/plot.py experiments/test_experiment/ --prompt-id P001 --output prompt_001.html
```

## 📊 Data Formats

### Generation Data Format

These files are generated by `infer` in `[experiment_name]/inference_data/prompt_XXX.json`. It contains information about the generations. There is one file per prompt, and it will contain all the generation for this prompt.

```json
{
  "prompt_metadata": {
    "prompt_id": "P0002",
    "creation_date": "2025-06-13T12:16:23Z",
    "prompt_text": "Lebron James is"
  },
  "model_info": {
    "model_name": "EleutherAI/pythia-70m-deduped",
    "max_length": 100,
    "temperature": 0.4,
    "top_k": 20,
    "top_p": 0.8,
    "repetition_penalty": 1
  },
  "tokenizer_info": {
    "tokenizer_name": "EleutherAI/pythia-70m-deduped",
    "vocab_size": 50254,
    "special_tokens": {
      "bos_token": "<|endoftext|>",
      "eos_token": "<|endoftext|>",
      "pad_token": null,
      "unk_token": "<|endoftext|>"
    }
  },
  "generations": [
    {
      "generation_id": "P0002_G00",
      "generated_text": " a member of the International ...",
      "token_ids": [
        247,
        3558,
        273,
        253, # etc.
      ],
      "token_texts": [
        " a",
        " member",
        " of",
        " the",
        " International", # etc....
      ],
      "token_perplexities": [
        1.0,
        1.9969673910624095,
        1.0,
        1.0,
        1.0, # etc. ...
      ],
      "generation_params": {
        "seed": 42,
        "timestamp": "2025-06-13T12:16:26Z"
      }
    }
  ]
}
```

### Low Perplexity Regions Format

This file is generated by `infer`, when extracting the low-perplexity windows from the generation. It will be later modified by `infinigram` to add the counts.

```json
{
  "source_prompt_id": "P0002",
  "analysis_metadata": {
    "analysis_date": "2025-06-13T12:16:23Z",
    "source_file": "inference_data/generations/prompt_002.json"
  },
  "analysis_parameters": {
    "window_size": 6,
    "perplexity_threshold": 1.0,
    "stride": 1
  },
  "per_prompt_regions": [
    {
      "generation_id": "P0002_G00",
      "per_gen_regions": [
        {
          "region_id": "P0002_G00_R01",
          "start_index": 19,
          "end_index": 25,
          "tokens": [
            " a",
            " member",
            " of",
            " the",
            " International",
            " League"
          ],
          "token_ids": [
            247,
            3558,
            273,
            253,
            5625,
            6884
          ],
          "is_contiguous": false,
          "is_in_prompt": false,
          "avg_perplexity": 1.0,
          "min_perplexity": 1.0,
          "max_perplexity": 1.0, # The info about the number of counts will go here as well, after running infinigram.
        }]
    }]
}
```

## 🛠️ Core src

### `infer.py` - Main Analysis Script

Generates text using language models and computes per-token perplexity with low-perplexity region extraction.

**Arguments:**
```bash
--model_name TEXT          # HuggingFace model name or path
--prompt TEXT              # Single prompt or JSON file path
--n_gen INTEGER            # Number of generations per prompt
--max_length INTEGER       # Maximum generation length
--temp FLOAT               # Sampling temperature
--window-size INTEGER      # Low-perplexity window size
--perplexity_threshold FLOAT # Perplexity threshold for region extraction
--stride INTEGER           # Sliding window stride
--experiment-name TEXT     # Experiment directory name
--verbose                  # Enable detailed logging
```

## 📝 Citation

If you use this toolkit in your research, please cite:

```bibtex
@software{perplexity_analysis_toolkit,
  title={Perplexity Analysis Toolkit},
  author={[Arthur Wuhrmann, Anastasiia Kucherenko]},
  year={2025},
  url={[https://github.com/Reliable-Information-Lab-HEVS/HAIDI-Graphs]}
}
```

## 🤝 Contributing

Contributions are welcome !

## 🙏 Acknowledgments

- [Infinigram](https://infini-gram.io/) for corpus frequency analysis
- [HuggingFace Transformers](https://huggingface.co/transformers/) for model inference
